{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14380092,"sourceType":"datasetVersion","datasetId":9183521}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport librosa as lb\nimport librosa.display\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers,models,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nimport os\nimport gc\nfrom PIL import Image as img\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:15:30.521844Z","iopub.execute_input":"2026-01-03T15:15:30.522014Z","iopub.status.idle":"2026-01-03T15:15:47.728667Z","shell.execute_reply.started":"2026-01-03T15:15:30.521996Z","shell.execute_reply":"2026-01-03T15:15:47.728019Z"}},"outputs":[{"name":"stderr","text":"2026-01-03 15:15:34.925670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767453335.153138      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767453335.214889      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767453335.741442      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767453335.741481      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767453335.741484      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767453335.741486      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**For the audio classification**\n<br>\n1) We take audio create 3 channeled image data for the audio\n2) Just like how we take rgb in the image we need 3 channels for the audio it would be<br>\n       i) Fourier transformation (more of frequency over time)stft<br>\n       ii) meft (image displaying frequencey that we humnas can listen to)<br>\n       iii)chroma(smaller pitches and notes)","metadata":{}},{"cell_type":"code","source":"\ndef format_shape(data, target_height=128, target_width=1000):\n    data = np.asarray(data, dtype=np.float32)\n\n    \n    # 1D -> 2D\n    if data.ndim == 1:\n        data = data[np.newaxis, :]\n\n    # Min-max normalization\n    data=(255 * (data - np.min(data)) / (np.max(data) - np.min(data))).astype(np.uint8)\n\n    # Resize rows to target_height\n    rows, cols = data.shape\n    if rows < target_height:\n        reps = int(np.ceil(target_height / rows))\n        data = np.tile(data, (reps, 1))[:target_height, :]\n    elif rows > target_height:\n        data = data[:target_height, :]\n\n    # Resize columns to target_width\n    rows, cols = data.shape\n    if cols < target_width:\n        pad_width = target_width - cols\n        data = np.pad(data, ((0,0), (0, pad_width)), mode=\"constant\")\n    elif cols > target_width:\n        data = data[:, :target_width]\n\n    return data.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:15:47.730150Z","iopub.execute_input":"2026-01-03T15:15:47.730614Z","iopub.status.idle":"2026-01-03T15:15:47.737171Z","shell.execute_reply.started":"2026-01-03T15:15:47.730589Z","shell.execute_reply":"2026-01-03T15:15:47.736401Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#this function will be responsible to convert all the audios into images\ndef audio_to_image(file=None,max_size=1000,y=None,sr=22050):\n    #loading up the image\n    if not file is None:\n        if y is None:\n            y,sr=librosa.load(file,sr=22050)\n    y = np.asarray(y, dtype=np.float32)\n\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n\n    #next channel which mfcc is which is audio graph that is audible to human\n    mels_db = librosa.power_to_db(mels, ref=np.max)\n    \n\n    \n    mels_delta = librosa.feature.delta(mels_db)\n    \n    mels_delta2 = librosa.feature.delta(mels_db, order=2)\n\n    def normalize(X):\n        x_min, x_max = X.min(), X.max()\n        if x_max - x_min > 0:\n            return (255 * (X - x_min) / (x_max - x_min)).astype(np.uint8)\n        return np.zeros_like(X, dtype=np.uint8)\n        \n    layer0 = format_shape(mels_db)     \n    layer1 = format_shape(mels_delta)  \n    layer2 = format_shape(mels_delta2)\n\n    #this makes cube by taking 3 images and stacking on top of each other\n    final_image = np.dstack([layer0, layer1, layer2]).astype(np.float32)\n    return final_image,y,sr\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:15:47.738286Z","iopub.execute_input":"2026-01-03T15:15:47.738604Z","iopub.status.idle":"2026-01-03T15:15:47.921619Z","shell.execute_reply.started":"2026-01-03T15:15:47.738569Z","shell.execute_reply":"2026-01-03T15:15:47.920875Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def convert_dir_to_audio(file_path, output_dir):\n    # Change 4: Only look for actual directories\n    class_names = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n\n    for i, class_name in enumerate(class_names):\n        class_input_path = os.path.join(file_path, class_name)\n        save_folder = os.path.join(output_dir, class_name)\n        os.makedirs(save_folder, exist_ok=True)\n        \n        print(f\"Processing class: {class_name}\")\n        \n        for file_name in os.listdir(class_input_path):\n            if file_name.endswith((\".wav\", \".m4a\")):\n                # Change 1: Separate the source path from the filename\n                full_file_path = os.path.join(class_input_path, file_name)\n\n                try:\n                    img, y, sr = audio_to_image(full_file_path, y=None)\n                    \n                    \n                    img_to_save = img.astype(np.uint8) \n                    im = Image.fromarray(img_to_save)\n                    im.save(os.path.join(save_folder, file_name + \".png\"))\n                    \n                    for j in range(3): \n                        y_aug = augment_audio(y, sr)\n                        img_aug, _, _ = audio_to_image(file=None, y=y_aug, sr=sr)\n                        img_aug_to_save = img_aug.astype(np.uint8) \n                        im_ = Image.fromarray(img_aug_to_save)\n                        im_.save(os.path.join(save_folder, file_name+i+ \".png\"))\n                        \n                        # Change 2: Clear memory inside the loop\n                        del y_aug, img_aug \n\n                    # Change 2: Clear memory outside the loop\n                    del img, y\n                    \n                except Exception as e:\n                    print(f\"Skipping {file_name}: {e}\")\n        \n        gc.collect()\n\n    # Change 3: Removed x=np.array(images) to save RAM\n    return class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:15:47.922518Z","iopub.execute_input":"2026-01-03T15:15:47.922849Z","iopub.status.idle":"2026-01-03T15:15:47.935720Z","shell.execute_reply.started":"2026-01-03T15:15:47.922815Z","shell.execute_reply":"2026-01-03T15:15:47.935003Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def augment_audio(y,sr):\n    if np.random.random()>0.5:\n        y=librosa.effects.pitch_shift(y,sr=sr,n_steps=np.random.uniform(-2,2))\n        y = np.asarray(y, dtype=np.float32)\n    noise_amp=0.005*np.random.uniform()*np.amax(y)\n    noise_amp = 0.005 * np.random.uniform() * np.amax(np.abs(y)) if y.size > 0 else 0.0\n    y = y + noise_amp * np.random.normal(size=y.shape).astype(np.float32)\n    return y.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:57:37.674738Z","iopub.execute_input":"2026-01-03T14:57:37.675400Z","iopub.status.idle":"2026-01-03T14:57:37.680029Z","shell.execute_reply.started":"2026-01-03T14:57:37.675370Z","shell.execute_reply":"2026-01-03T14:57:37.679260Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def split_data(\n    x,\n    y,\n    test_size=0.2,\n    val_size=0.5,\n    random_state=42,\n    stratify=True\n):\n    x = np.array(x)\n    y = np.array(y)\n\n    strat = y if stratify else None\n\n    # Train / temp split\n    x_train, x_temp, y_train, y_temp = train_test_split(\n        x,\n        y,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=strat\n    )\n\n    # Val / test split\n    x_val, x_test, y_val, y_test = train_test_split(\n        x_temp,\n        y_temp,\n        test_size=val_size,\n        random_state=random_state,\n        stratify=(y_temp if stratify else None)\n    )\n\n    return x_train, y_train, x_val, y_val, x_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:57:39.489327Z","iopub.execute_input":"2026-01-03T14:57:39.489608Z","iopub.status.idle":"2026-01-03T14:57:39.494648Z","shell.execute_reply.started":"2026-01-03T14:57:39.489584Z","shell.execute_reply":"2026-01-03T14:57:39.494002Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def for_single_audio(file):\n    img,y,sr=audio_to_image(file)\n    img_array=np.expand_dims(img_array, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:57:39.680536Z","iopub.execute_input":"2026-01-03T14:57:39.680787Z","iopub.status.idle":"2026-01-03T14:57:39.684916Z","shell.execute_reply.started":"2026-01-03T14:57:39.680763Z","shell.execute_reply":"2026-01-03T14:57:39.684172Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"input=\"/kaggle/input/forest-sound-dataset/forestdataset\"\noutput=\"/kaggle/working/processed\"\nclass_names=convert_dir_to_audio(input,output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:57:41.196115Z","iopub.execute_input":"2026-01-03T14:57:41.196605Z","iopub.status.idle":"2026-01-03T15:06:33.161659Z","shell.execute_reply.started":"2026-01-03T14:57:41.196575Z","shell.execute_reply":"2026-01-03T15:06:33.161057Z"}},"outputs":[{"name":"stdout","text":"Processing class: logging\nProcessing class: poaching\nProcessing class: natural sound\nProcessing class: fire\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2832389863.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n  y,sr=librosa.load(file,sr=22050)\n/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/processed\", 'zip', \"/kaggle/working/processed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:07:51.095034Z","iopub.execute_input":"2026-01-03T15:07:51.095733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#now we take pretrained model and train them for our data\n\ndata_augmentation=tf.keras.Sequential([\n     tf.keras.layers.RandomTranslation(\n        height_factor=0.0,  # no vertical shift by default\n        width_factor=0.1,   # shift up to 10% horizontally\n        fill_mode='constant'\n    ),\n\n    # Random vertical shift (frequency axis)\n    tf.keras.layers.RandomTranslation(\n        height_factor=0.1,  # shift up to 10% vertically\n        width_factor=0.0,\n        fill_mode='constant'\n    ),\n\n    # Random brightness adjustment (small)\n    tf.keras.layers.RandomBrightness(factor=0.1),\n\n    # Random contrast adjustment\n    tf.keras.layers.RandomContrast(factor=0.1),\n\n    # Random zoom\n    tf.keras.layers.RandomZoom(height_factor=(-0.1,0.1), width_factor=(-0.1,0.1))\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:50:49.465325Z","iopub.status.idle":"2026-01-03T14:50:49.465645Z","shell.execute_reply.started":"2026-01-03T14:50:49.465488Z","shell.execute_reply":"2026-01-03T14:50:49.465507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mobile=tf.keras.applications.EfficientNetB0(\n    input_shape=(224,224,3),\n    include_top=False,\n    weights=\"imagenet\"\n)\nmobile.trainable=False\n\nmodel=models.Sequential([\n    tf.keras.layers.Input(shape=(128,1000,3)),\n    data_augmentation,\n    tf.keras.layers.Resizing(224,224),\n\n    #mobile net takes values from -1 to 1 so\n    layers.Lambda(tf.keras.applications.mobilenet_v2.preprocess_input),\n    mobile,\n\n    tf.keras.layers.GlobalMaxPool2D(),\n  \n    tf.keras.layers.Dense(256, activation=\"relu\"),\n    tf.keras.layers.Dense(4,activation=\"softmax\")\n    \n    \n    \n    \n])\n\n\nmodel.compile(optimizer=\"adam\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhistory=model.fit(\n    x_train,y_train,\n    epochs=50,\n    batch_size=32,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\ntest_loss,test_acc=model.evaluate(x_val,y_val)\nprint(f\"Accuracy: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:50:49.466260Z","iopub.status.idle":"2026-01-03T14:50:49.466582Z","shell.execute_reply.started":"2026-01-03T14:50:49.466401Z","shell.execute_reply":"2026-01-03T14:50:49.466436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result=model.evaluate(x_test,y_test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:41:03.307535Z","iopub.execute_input":"2026-01-03T14:41:03.307767Z","iopub.status.idle":"2026-01-03T14:41:05.195451Z","shell.execute_reply.started":"2026-01-03T14:41:03.307743Z","shell.execute_reply":"2026-01-03T14:41:05.194696Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.5251 - loss: 1.1448\nFor the unseen test data of the entire training this model has accuracy of 0.5312\\\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_forest.keras\")\nimport shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T14:41:05.196396Z","iopub.execute_input":"2026-01-03T14:41:05.196665Z","iopub.status.idle":"2026-01-03T14:41:06.838460Z","shell.execute_reply.started":"2026-01-03T14:41:05.196634Z","shell.execute_reply":"2026-01-03T14:41:06.837666Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/models.zip'"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"**No of files:**\n<br>\n1) Fire=336<br>\n2) Logging=455<br>\n3) Natural=1063<br>\n4) Poaching=531<br>\n5) Total= 2385<br>\nSo, threshold accuracy=(531/2385)*100%=22.26%","metadata":{}}]}