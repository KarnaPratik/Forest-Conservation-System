{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14380092,"sourceType":"datasetVersion","datasetId":9183521},{"sourceId":14383382,"sourceType":"datasetVersion","datasetId":9185806}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport librosa as lb\nimport librosa.display\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers,models,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nimport os\nimport gc\nfrom PIL import Image as PIL_Image\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:25:11.473454Z","iopub.execute_input":"2026-01-03T17:25:11.474338Z","iopub.status.idle":"2026-01-03T17:25:11.479527Z","shell.execute_reply.started":"2026-01-03T17:25:11.474304Z","shell.execute_reply":"2026-01-03T17:25:11.478864Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**For the audio classification**\n<br>\n1) We take audio create 3 channeled image data for the audio\n2) Just like how we take rgb in the image we need 3 channels for the audio it would be<br>\n       i) Fourier transformation (more of frequency over time)stft<br>\n       ii) meft (image displaying frequencey that we humnas can listen to)<br>\n       iii)chroma(smaller pitches and notes)","metadata":{}},{"cell_type":"code","source":"\ndef format_shape(data, target_height=128, target_width=1000):\n    data = np.asarray(data, dtype=np.float32)\n\n    \n    # 1D -> 2D\n    if data.ndim == 1:\n        data = data[np.newaxis, :]\n\n    # Min-max normalization\n    data=(255 * (data - np.min(data)) / (np.max(data) - np.min(data))).astype(np.uint8)\n\n    # Resize rows to target_height\n    rows, cols = data.shape\n    if rows < target_height:\n        reps = int(np.ceil(target_height / rows))\n        data = np.tile(data, (reps, 1))[:target_height, :]\n    elif rows > target_height:\n        data = data[:target_height, :]\n\n    # Resize columns to target_width\n    rows, cols = data.shape\n    if cols < target_width:\n        pad_width = target_width - cols\n        data = np.pad(data, ((0,0), (0, pad_width)), mode=\"constant\")\n    elif cols > target_width:\n        data = data[:, :target_width]\n\n    return data.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:10.512650Z","iopub.execute_input":"2026-01-03T15:42:10.513502Z","iopub.status.idle":"2026-01-03T15:42:10.519636Z","shell.execute_reply.started":"2026-01-03T15:42:10.513471Z","shell.execute_reply":"2026-01-03T15:42:10.518959Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#this function will be responsible to convert all the audios into images\ndef audio_to_image(file=None,max_size=1000,y=None,sr=22050):\n    #loading up the image\n    if not file is None:\n        if y is None:\n            y,sr=librosa.load(file,sr=22050)\n    y = np.asarray(y, dtype=np.float32)\n\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n\n    #next channel which mfcc is which is audio graph that is audible to human\n    mels_db = librosa.power_to_db(mels, ref=np.max)\n    \n\n    \n    mels_delta = librosa.feature.delta(mels_db)\n    \n    mels_delta2 = librosa.feature.delta(mels_db, order=2)\n\n    def normalize(X):\n        x_min, x_max = X.min(), X.max()\n        if x_max - x_min > 0:\n            return (255 * (X - x_min) / (x_max - x_min)).astype(np.uint8)\n        return np.zeros_like(X, dtype=np.uint8)\n        \n    layer0 = format_shape(mels_db)     \n    layer1 = format_shape(mels_delta)  \n    layer2 = format_shape(mels_delta2)\n\n    #this makes cube by taking 3 images and stacking on top of each other\n    final_image = np.dstack([layer0, layer1, layer2]).astype(np.float32)\n    return final_image,y,sr\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:16.655519Z","iopub.execute_input":"2026-01-03T15:42:16.656249Z","iopub.status.idle":"2026-01-03T15:42:16.662560Z","shell.execute_reply.started":"2026-01-03T15:42:16.656217Z","shell.execute_reply":"2026-01-03T15:42:16.661740Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def convert_dir_to_audio(file_path, output_dir):\n    class_names = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n\n    for i, class_name in enumerate(class_names):\n        class_input_path = os.path.join(file_path, class_name)\n        save_folder = os.path.join(output_dir, class_name)\n        os.makedirs(save_folder, exist_ok=True)\n        \n        print(f\"--- Processing Class: {class_name} ---\")\n        \n        for file_name in os.listdir(class_input_path):\n            if file_name.endswith((\".wav\", \".m4a\")):\n                full_file_path = os.path.join(class_input_path, file_name)\n\n                try:\n                    # 1. Process Original\n                    # Note: We use 'pixel_data' everywhere now\n                    pixel_data, y, sr = audio_to_image(full_file_path, y=None)\n                    \n                    # Save Original\n                    im = PIL_Image.fromarray(pixel_data.astype(np.uint8))\n                    im.save(os.path.join(save_folder, f\"{file_name}_orig.png\"))\n                    \n                    # 2. Process Augmentations\n                    for j in range(3): \n                        y_aug = augment_audio(y, sr)\n                        aug_pixel_data, _, _ = audio_to_image(file=None, y=y_aug, sr=sr)\n                        \n                        im_aug = PIL_Image.fromarray(aug_pixel_data.astype(np.uint8))\n                        # Use f-string to safely combine string and number j\n                        im_aug.save(os.path.join(save_folder, f\"{file_name}_aug_{j}.png\"))\n                        \n                        del y_aug, aug_pixel_data \n\n                    # Cleanup the original data for this file\n                    del pixel_data, y\n                    \n                except Exception as e:\n                    print(f\"Skipping {file_name}: {e}\")\n        \n        gc.collect()\n\n    return class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:19.014503Z","iopub.execute_input":"2026-01-03T15:42:19.015210Z","iopub.status.idle":"2026-01-03T15:42:19.022055Z","shell.execute_reply.started":"2026-01-03T15:42:19.015178Z","shell.execute_reply":"2026-01-03T15:42:19.021281Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def augment_audio(y,sr):\n    if np.random.random()>0.5:\n        y=librosa.effects.pitch_shift(y,sr=sr,n_steps=np.random.uniform(-2,2))\n        y = np.asarray(y, dtype=np.float32)\n    noise_amp=0.005*np.random.uniform()*np.amax(y)\n    noise_amp = 0.005 * np.random.uniform() * np.amax(np.abs(y)) if y.size > 0 else 0.0\n    y = y + noise_amp * np.random.normal(size=y.shape).astype(np.float32)\n    return y.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:23.050130Z","iopub.execute_input":"2026-01-03T15:42:23.050892Z","iopub.status.idle":"2026-01-03T15:42:23.055859Z","shell.execute_reply.started":"2026-01-03T15:42:23.050859Z","shell.execute_reply":"2026-01-03T15:42:23.055044Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def split_data(\n    x,\n    y,\n    test_size=0.2,\n    val_size=0.5,\n    random_state=42,\n    stratify=True\n):\n    x = np.array(x)\n    y = np.array(y)\n\n    strat = y if stratify else None\n\n    # Train / temp split\n    x_train, x_temp, y_train, y_temp = train_test_split(\n        x,\n        y,\n        test_size=test_size,\n        random_state=random_state,\n        stratify=strat\n    )\n\n    # Val / test split\n    x_val, x_test, y_val, y_test = train_test_split(\n        x_temp,\n        y_temp,\n        test_size=val_size,\n        random_state=random_state,\n        stratify=(y_temp if stratify else None)\n    )\n\n    return x_train, y_train, x_val, y_val, x_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:23.876269Z","iopub.execute_input":"2026-01-03T15:42:23.876810Z","iopub.status.idle":"2026-01-03T15:42:23.881521Z","shell.execute_reply.started":"2026-01-03T15:42:23.876767Z","shell.execute_reply":"2026-01-03T15:42:23.880836Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def for_single_audio(file):\n    img,y,sr=audio_to_image(file)\n    img_array=np.expand_dims(img_array, axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:26.970367Z","iopub.execute_input":"2026-01-03T15:42:26.970963Z","iopub.status.idle":"2026-01-03T15:42:26.974495Z","shell.execute_reply.started":"2026-01-03T15:42:26.970938Z","shell.execute_reply":"2026-01-03T15:42:26.973851Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"input=\"/kaggle/input/forest-sound-dataset/forestdataset\"\noutput=\"/kaggle/working/processed\"\nclass_names=convert_dir_to_audio(input,output)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:42:36.978629Z","iopub.execute_input":"2026-01-03T15:42:36.979221Z","iopub.status.idle":"2026-01-03T15:53:28.264021Z","shell.execute_reply.started":"2026-01-03T15:42:36.979190Z","shell.execute_reply":"2026-01-03T15:53:28.263192Z"}},"outputs":[{"name":"stdout","text":"--- Processing Class: logging ---\n--- Processing Class: poaching ---\n--- Processing Class: natural sound ---\n--- Processing Class: fire ---\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2832389863.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n  y,sr=librosa.load(file,sr=22050)\n/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/forest_processed\", 'zip', \"/kaggle/working/processed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T15:58:45.488752Z","iopub.execute_input":"2026-01-03T15:58:45.489507Z","iopub.status.idle":"2026-01-03T15:59:06.108215Z","shell.execute_reply.started":"2026-01-03T15:58:45.489478Z","shell.execute_reply":"2026-01-03T15:59:06.107501Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/forest_processed.zip'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:25:34.581635Z","iopub.execute_input":"2026-01-03T17:25:34.581939Z","iopub.status.idle":"2026-01-03T17:25:34.585818Z","shell.execute_reply.started":"2026-01-03T17:25:34.581913Z","shell.execute_reply":"2026-01-03T17:25:34.585086Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"#setting up important var for loading images later on\nBATCH_SIZE=32\nIMG_SIZE=(224,224)\nSEED=42\nEXTRACT_PATH=\"/kaggle/input/forest-sound-spectograph/forest_processed\"\nselected_class=[\"natural sound\",\"unnatural\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:25:38.165167Z","iopub.execute_input":"2026-01-03T17:25:38.165690Z","iopub.status.idle":"2026-01-03T17:25:38.169317Z","shell.execute_reply.started":"2026-01-03T17:25:38.165661Z","shell.execute_reply":"2026-01-03T17:25:38.168601Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\" \n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:25:40.586084Z","iopub.execute_input":"2026-01-03T16:25:40.586561Z","iopub.status.idle":"2026-01-03T16:26:00.593088Z","shell.execute_reply.started":"2026-01-03T16:25:40.586533Z","shell.execute_reply":"2026-01-03T16:26:00.592498Z"}},"outputs":[{"name":"stdout","text":"Found 8016 files belonging to 2 classes.\nUsing 6413 files for training.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767457556.022727      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1767457556.026719      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Found 8016 files belonging to 2 classes.\nUsing 1603 files for validation.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:26:17.824224Z","iopub.execute_input":"2026-01-03T16:26:17.824788Z","iopub.status.idle":"2026-01-03T16:26:17.834388Z","shell.execute_reply.started":"2026-01-03T16:26:17.824744Z","shell.execute_reply":"2026-01-03T16:26:17.833593Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:26:35.047353Z","iopub.execute_input":"2026-01-03T16:26:35.047679Z","iopub.status.idle":"2026-01-03T16:26:35.052822Z","shell.execute_reply.started":"2026-01-03T16:26:35.047650Z","shell.execute_reply":"2026-01-03T16:26:35.052013Z"}},"outputs":[{"name":"stdout","text":"Class names: ['natural sound', 'unnatural']\nFor values:\n\nnatural sound:0\n\nunnatural:1\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"#now we take pretrained model and train them for our data\n\ndata_augmentation=tf.keras.Sequential([\n     tf.keras.layers.RandomTranslation(\n        height_factor=0.0,  # no vertical shift by default\n        width_factor=0.1,   # shift up to 10% horizontally\n        fill_mode='constant'\n    ),\n\n    # Random vertical shift (frequency axis)\n    tf.keras.layers.RandomTranslation(\n        height_factor=0.1,  # shift up to 10% vertically\n        width_factor=0.0,\n        fill_mode='constant'\n    ),\n\n    # Random brightness adjustment (small)\n    tf.keras.layers.RandomBrightness(factor=0.1),\n\n    # Random contrast adjustment\n    tf.keras.layers.RandomContrast(factor=0.1),\n\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:29:29.306972Z","iopub.execute_input":"2026-01-03T16:29:29.307561Z","iopub.status.idle":"2026-01-03T16:29:29.333662Z","shell.execute_reply.started":"2026-01-03T16:29:29.307531Z","shell.execute_reply":"2026-01-03T16:29:29.332967Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"mobile=tf.keras.applications.EfficientNetB0(\n    input_shape=(224,224,3),\n    include_top=False,\n    weights=\"imagenet\"\n)\nmobile.trainable=False\n\nmodel=models.Sequential([\n    tf.keras.layers.Input(shape=(128,1000,3)),\n    data_augmentation,\n    tf.keras.layers.Resizing(224,224),\n\n    #mobile net takes values from -1 to 1 so\n    tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input),\n    mobile,\n\n    tf.keras.layers.GlobalAveragePooling2D(),\n  \n    tf.keras.layers.Dense(256, activation=\"relu\"),\n\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Dense(1,activation=\"sigmoid\")\n    \n    \n    \n    \n])\n\n\nmodel.compile(optimizer=\"adam\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhistory=model.fit(\n    train_data,\n    validation_data=validation,\n    epochs= 20,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\n\ntest_loss,test_acc=model.evaluate(validation)\nprint(f\"Accuracy: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:49:07.568531Z","iopub.execute_input":"2026-01-03T16:49:07.569306Z","iopub.status.idle":"2026-01-03T16:55:11.670285Z","shell.execute_reply.started":"2026-01-03T16:49:07.569274Z","shell.execute_reply":"2026-01-03T16:55:11.669667Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1767458956.925758      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/sequential_4_1/efficientnetb0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7492 - loss: 0.5007\nEpoch 1: val_accuracy improved from -inf to 0.80375, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - accuracy: 0.7495 - loss: 0.5002 - val_accuracy: 0.8037 - val_loss: 0.4074\nEpoch 2/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.7965 - loss: 0.4238\nEpoch 2: val_accuracy improved from 0.80375 to 0.81375, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 90ms/step - accuracy: 0.7966 - loss: 0.4236 - val_accuracy: 0.8138 - val_loss: 0.3970\nEpoch 3/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8058 - loss: 0.4092\nEpoch 3: val_accuracy improved from 0.81375 to 0.81625, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8059 - loss: 0.4089 - val_accuracy: 0.8163 - val_loss: 0.3846\nEpoch 4/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8187 - loss: 0.3863\nEpoch 4: val_accuracy improved from 0.81625 to 0.82375, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 90ms/step - accuracy: 0.8189 - loss: 0.3861 - val_accuracy: 0.8238 - val_loss: 0.3737\nEpoch 5/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8203 - loss: 0.3830\nEpoch 5: val_accuracy improved from 0.82375 to 0.82750, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.8204 - loss: 0.3828 - val_accuracy: 0.8275 - val_loss: 0.3645\nEpoch 6/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8198 - loss: 0.3784\nEpoch 6: val_accuracy improved from 0.82750 to 0.84000, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8199 - loss: 0.3783 - val_accuracy: 0.8400 - val_loss: 0.3498\nEpoch 7/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8219 - loss: 0.3712\nEpoch 7: val_accuracy did not improve from 0.84000\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step - accuracy: 0.8220 - loss: 0.3711 - val_accuracy: 0.8375 - val_loss: 0.3479\nEpoch 8/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8352 - loss: 0.3625\nEpoch 8: val_accuracy did not improve from 0.84000\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step - accuracy: 0.8352 - loss: 0.3624 - val_accuracy: 0.8338 - val_loss: 0.3499\nEpoch 9/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8331 - loss: 0.3524\nEpoch 9: val_accuracy improved from 0.84000 to 0.84250, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8332 - loss: 0.3523 - val_accuracy: 0.8425 - val_loss: 0.3411\nEpoch 10/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8355 - loss: 0.3476\nEpoch 10: val_accuracy improved from 0.84250 to 0.84625, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.8355 - loss: 0.3476 - val_accuracy: 0.8462 - val_loss: 0.3415\nEpoch 11/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8392 - loss: 0.3585\nEpoch 11: val_accuracy did not improve from 0.84625\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - accuracy: 0.8392 - loss: 0.3584 - val_accuracy: 0.8425 - val_loss: 0.3244\nEpoch 12/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8321 - loss: 0.3552\nEpoch 12: val_accuracy did not improve from 0.84625\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - accuracy: 0.8322 - loss: 0.3551 - val_accuracy: 0.8413 - val_loss: 0.3487\nEpoch 13/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8530 - loss: 0.3431\nEpoch 13: val_accuracy improved from 0.84625 to 0.85875, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8530 - loss: 0.3429 - val_accuracy: 0.8587 - val_loss: 0.3207\nEpoch 14/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8372 - loss: 0.3369\nEpoch 14: val_accuracy improved from 0.85875 to 0.86125, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8373 - loss: 0.3367 - val_accuracy: 0.8612 - val_loss: 0.3094\nEpoch 15/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8488 - loss: 0.3388\nEpoch 15: val_accuracy improved from 0.86125 to 0.87250, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8489 - loss: 0.3387 - val_accuracy: 0.8725 - val_loss: 0.3000\nEpoch 16/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8511 - loss: 0.3347\nEpoch 16: val_accuracy improved from 0.87250 to 0.87750, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8512 - loss: 0.3345 - val_accuracy: 0.8775 - val_loss: 0.3073\nEpoch 17/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8447 - loss: 0.3389\nEpoch 17: val_accuracy did not improve from 0.87750\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - accuracy: 0.8448 - loss: 0.3388 - val_accuracy: 0.8662 - val_loss: 0.3121\nEpoch 18/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8550 - loss: 0.3303\nEpoch 18: val_accuracy did not improve from 0.87750\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - accuracy: 0.8551 - loss: 0.3301 - val_accuracy: 0.8712 - val_loss: 0.2982\nEpoch 19/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8552 - loss: 0.3283\nEpoch 19: val_accuracy improved from 0.87750 to 0.88250, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8552 - loss: 0.3282 - val_accuracy: 0.8825 - val_loss: 0.2911\nEpoch 20/20\n\u001b[1m200/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8568 - loss: 0.3243\nEpoch 20: val_accuracy did not improve from 0.88250\n\u001b[1m201/201\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 84ms/step - accuracy: 0.8568 - loss: 0.3242 - val_accuracy: 0.8775 - val_loss: 0.2961\n\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.8694 - loss: 0.2919\nAccuracy: 0.882500\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"result=model.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:56:01.924133Z","iopub.execute_input":"2026-01-03T16:56:01.924869Z","iopub.status.idle":"2026-01-03T16:56:05.225455Z","shell.execute_reply.started":"2026-01-03T16:56:01.924838Z","shell.execute_reply":"2026-01-03T16:56:05.224922Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 0.8909 - loss: 0.2703\nFor the unseen test data of the entire training this model has accuracy of 0.8842\\\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"natural 4252\nunnatural 3764\n\nmodel accuracy = 88.48","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_forest.keras\")\nimport shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T16:56:14.818475Z","iopub.execute_input":"2026-01-03T16:56:14.819171Z","iopub.status.idle":"2026-01-03T16:56:16.391970Z","shell.execute_reply.started":"2026-01-03T16:56:14.819144Z","shell.execute_reply":"2026-01-03T16:56:16.391228Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/models.zip'"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"**No of files:**\n<br>\n1) Fire=336<br>\n2) Logging=455<br>\n3) Natural=1063<br>\n4) Poaching=531<br>\n5) Total= 2385<br>\nSo, threshold accuracy=(531/2385)*100%=22.26%","metadata":{}},{"cell_type":"code","source":"selected_class=[\"fire\",\"logging\", \"poaching\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:25:55.976104Z","iopub.execute_input":"2026-01-03T17:25:55.976371Z","iopub.status.idle":"2026-01-03T17:25:55.979911Z","shell.execute_reply.started":"2026-01-03T17:25:55.976351Z","shell.execute_reply":"2026-01-03T17:25:55.979253Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"int\" \n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    image_size=IMG_SIZE,\n    batch_size=BATCH_SIZE,\n    label_mode=\"int\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:25:59.046440Z","iopub.execute_input":"2026-01-03T17:25:59.046928Z","iopub.status.idle":"2026-01-03T17:26:02.108212Z","shell.execute_reply.started":"2026-01-03T17:25:59.046900Z","shell.execute_reply":"2026-01-03T17:26:02.107465Z"}},"outputs":[{"name":"stdout","text":"Found 3764 files belonging to 3 classes.\nUsing 3012 files for training.\nFound 3764 files belonging to 3 classes.\nUsing 752 files for validation.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:26:07.076047Z","iopub.execute_input":"2026-01-03T17:26:07.076606Z","iopub.status.idle":"2026-01-03T17:26:07.083853Z","shell.execute_reply.started":"2026-01-03T17:26:07.076580Z","shell.execute_reply":"2026-01-03T17:26:07.083297Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"print(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:26:11.067542Z","iopub.execute_input":"2026-01-03T17:26:11.068135Z","iopub.status.idle":"2026-01-03T17:26:11.072513Z","shell.execute_reply.started":"2026-01-03T17:26:11.068109Z","shell.execute_reply":"2026-01-03T17:26:11.071812Z"}},"outputs":[{"name":"stdout","text":"Class names: ['fire', 'logging', 'poaching']\nFor values:\n\nfire:0\n\nlogging:1\n\npoaching:2\n\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"mobile2=tf.keras.applications.EfficientNetB0(\n    input_shape=(224,224,3),\n    include_top=False,\n    weights=\"imagenet\"\n)\nmobile2.trainable=False\n\nmodel=models.Sequential([\n    tf.keras.layers.Input(shape=(128,1000,3)),\n    data_augmentation,\n    tf.keras.layers.Resizing(224,224),\n\n    #mobile net takes values from -1 to 1 so\n    tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input),\n    mobile2,\n\n    tf.keras.layers.GlobalAveragePooling2D(),\n  \n    tf.keras.layers.Dense(256, activation=\"relu\"),\n\n    tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Dense(3,activation=\"softmax\")\n    \n    \n    \n    \n])\n\n\nmodel.compile(optimizer=\"adam\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhistory=model.fit(\n    train_data,\n    validation_data=validation,\n    epochs= 20,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\n\ntest_loss,test_acc=model.evaluate(validation)\nprint(f\"Accuracy: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:26:20.165324Z","iopub.execute_input":"2026-01-03T17:26:20.165969Z","iopub.status.idle":"2026-01-03T17:27:59.411421Z","shell.execute_reply.started":"2026-01-03T17:26:20.165942Z","shell.execute_reply":"2026-01-03T17:27:59.410827Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1767461190.548493      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/sequential_9_1/efficientnetb0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6993 - loss: 0.6823\nEpoch 1: val_accuracy improved from -inf to 0.81510, saving model to best_model.keras\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.7003 - loss: 0.6802 - val_accuracy: 0.8151 - val_loss: 0.5361\nEpoch 2/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8098 - loss: 0.4634\nEpoch 2: val_accuracy improved from 0.81510 to 0.86198, saving model to best_model.keras\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.8098 - loss: 0.4632 - val_accuracy: 0.8620 - val_loss: 0.3935\nEpoch 3/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8277 - loss: 0.4067\nEpoch 3: val_accuracy did not improve from 0.86198\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.8277 - loss: 0.4065 - val_accuracy: 0.8438 - val_loss: 0.3851\nEpoch 4/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8252 - loss: 0.3891\nEpoch 4: val_accuracy improved from 0.86198 to 0.86979, saving model to best_model.keras\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 92ms/step - accuracy: 0.8254 - loss: 0.3890 - val_accuracy: 0.8698 - val_loss: 0.3477\nEpoch 5/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8468 - loss: 0.3678\nEpoch 5: val_accuracy improved from 0.86979 to 0.88542, saving model to best_model.keras\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 91ms/step - accuracy: 0.8467 - loss: 0.3679 - val_accuracy: 0.8854 - val_loss: 0.2803\nEpoch 6/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8424 - loss: 0.3563\nEpoch 6: val_accuracy did not improve from 0.88542\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step - accuracy: 0.8426 - loss: 0.3563 - val_accuracy: 0.8594 - val_loss: 0.3419\nEpoch 7/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8643 - loss: 0.3532\nEpoch 7: val_accuracy did not improve from 0.88542\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step - accuracy: 0.8641 - loss: 0.3531 - val_accuracy: 0.8802 - val_loss: 0.2873\nEpoch 8/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8635 - loss: 0.3444\nEpoch 8: val_accuracy did not improve from 0.88542\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.8634 - loss: 0.3446 - val_accuracy: 0.8828 - val_loss: 0.3167\nEpoch 9/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8512 - loss: 0.3665\nEpoch 9: val_accuracy did not improve from 0.88542\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step - accuracy: 0.8513 - loss: 0.3662 - val_accuracy: 0.8724 - val_loss: 0.2987\nEpoch 10/20\n\u001b[1m94/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8657 - loss: 0.3404\nEpoch 10: val_accuracy did not improve from 0.88542\n\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.8657 - loss: 0.3403 - val_accuracy: 0.8854 - val_loss: 0.3057\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 75ms/step - accuracy: 0.8728 - loss: 0.3264\nAccuracy: 0.880208\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"result=model.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:28:58.002152Z","iopub.execute_input":"2026-01-03T17:28:58.002725Z","iopub.status.idle":"2026-01-03T17:28:59.849351Z","shell.execute_reply.started":"2026-01-03T17:28:58.002697Z","shell.execute_reply":"2026-01-03T17:28:59.848761Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 110ms/step - accuracy: 0.8803 - loss: 0.3052\nFor the unseen test data of the entire training this model has accuracy of 0.8750\\\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"fire - 1344\npoaching - 600\nlogging - 1820\n\ntotal = 3764\n\nthreshold accuracy = 48.35\n\nmodel 2 accuracy = 87.50","metadata":{}},{"cell_type":"code","source":"os.makedirs(\"/kaggle/working/models\",exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_multi_classification.keras\")\nimport shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/maheshdalle\", 'zip', \"/kaggle/working/models\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T17:32:12.132241Z","iopub.execute_input":"2026-01-03T17:32:12.132599Z","iopub.status.idle":"2026-01-03T17:32:14.695453Z","shell.execute_reply.started":"2026-01-03T17:32:12.132573Z","shell.execute_reply":"2026-01-03T17:32:14.694720Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/maheshdalle.zip'"},"metadata":{}}],"execution_count":51}]}