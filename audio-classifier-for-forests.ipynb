{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14383382,"sourceType":"datasetVersion","datasetId":9185806}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport librosa as lb\nimport librosa.display\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras import layers,models,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport tensorflow as tf\nimport os\nimport gc\nfrom PIL import Image as PIL_Image\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:22.048396Z","iopub.execute_input":"2026-01-04T05:06:22.049141Z","iopub.status.idle":"2026-01-04T05:06:28.278867Z","shell.execute_reply.started":"2026-01-04T05:06:22.049110Z","shell.execute_reply":"2026-01-04T05:06:28.278264Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**For the audio classification**\n<br>\n1) We take audio create 3 channeled image data for the audio\n2) Just like how we take rgb in the image we need 3 channels for the audio it would be<br>\n       i) Fourier transformation (more of frequency over time)stft<br>\n       ii) meft (image displaying frequencey that we humnas can listen to)<br>\n       iii)chroma(smaller pitches and notes)","metadata":{}},{"cell_type":"code","source":"\ndef format_shape(data, target_height=128, target_width=1000):\n    data = np.asarray(data, dtype=np.float32)\n\n    \n    # 1D -> 2D\n    if data.ndim == 1:\n        data = data[np.newaxis, :]\n\n    # Min-max normalization\n    data=(255 * (data - np.min(data)) / (np.max(data) - np.min(data))).astype(np.uint8)\n\n    # Resize rows to target_height\n    rows, cols = data.shape\n    if rows < target_height:\n        reps = int(np.ceil(target_height / rows))\n        data = np.tile(data, (reps, 1))[:target_height, :]\n    elif rows > target_height:\n        data = data[:target_height, :]\n\n    # Resize columns to target_width\n    rows, cols = data.shape\n    if cols < target_width:\n        pad_width = target_width - cols\n        data = np.pad(data, ((0,0), (0, pad_width)), mode=\"constant\")\n    elif cols > target_width:\n        data = data[:, :target_width]\n\n    return data.astype(np.float32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:18.519570Z","iopub.status.idle":"2026-01-04T05:06:18.519806Z","shell.execute_reply.started":"2026-01-04T05:06:18.519694Z","shell.execute_reply":"2026-01-04T05:06:18.519708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#this function will be responsible to convert all the audios into images\ndef audio_to_image(file=None,max_size=1000,y=None,sr=22050):\n    #loading up the image\n    if not file is None:\n        if y is None:\n            y,sr=librosa.load(file,sr=22050)\n    y = np.asarray(y, dtype=np.float32)\n\n    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, n_fft=2048, hop_length=512)\n\n    #next channel which mfcc is which is audio graph that is audible to human\n    mels_db = librosa.power_to_db(mels, ref=np.max)\n    \n\n    \n    mels_delta = librosa.feature.delta(mels_db)\n    \n    mels_delta2 = librosa.feature.delta(mels_db, order=2)\n\n    def normalize(X):\n        x_min, x_max = X.min(), X.max()\n        if x_max - x_min > 0:\n            return (255 * (X - x_min) / (x_max - x_min)).astype(np.uint8)\n        return np.zeros_like(X, dtype=np.uint8)\n        \n    layer0 = format_shape(mels_db)     \n    layer1 = format_shape(mels_delta)  \n    layer2 = format_shape(mels_delta2)\n\n    #this makes cube by taking 3 images and stacking on top of each other\n    final_image = np.dstack([layer0, layer1, layer2]).astype(np.float32)\n    return final_image,y,sr\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T04:11:12.860948Z","iopub.status.idle":"2026-01-04T04:11:12.861219Z","shell.execute_reply.started":"2026-01-04T04:11:12.861071Z","shell.execute_reply":"2026-01-04T04:11:12.861084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_dir_to_audio(file_path, output_dir):\n    class_names = [d for d in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, d))]\n\n    for i, class_name in enumerate(class_names):\n        class_input_path = os.path.join(file_path, class_name)\n        save_folder = os.path.join(output_dir, class_name)\n        os.makedirs(save_folder, exist_ok=True)\n        \n        print(f\"--- Processing Class: {class_name} ---\")\n        \n        for file_name in os.listdir(class_input_path):\n            if file_name.endswith((\".wav\", \".m4a\")):\n                full_file_path = os.path.join(class_input_path, file_name)\n\n                try:\n                    # 1. Process Original\n                    # Note: We use 'pixel_data' everywhere now\n                    pixel_data, y, sr = audio_to_image(full_file_path, y=None)\n                    \n                    # Save Original\n                    im = PIL_Image.fromarray(pixel_data.astype(np.uint8))\n                    im.save(os.path.join(save_folder, f\"{file_name}_orig.png\"))\n                    \n                    # 2. Process Augmentations\n                    for j in range(3): \n                        y_aug = augment_audio(y, sr)\n                        aug_pixel_data, _, _ = audio_to_image(file=None, y=y_aug, sr=sr)\n                        \n                        im_aug = PIL_Image.fromarray(aug_pixel_data.astype(np.uint8))\n                        # Use f-string to safely combine string and number j\n                        im_aug.save(os.path.join(save_folder, f\"{file_name}_aug_{j}.png\"))\n                        \n                        del y_aug, aug_pixel_data \n\n                    # Cleanup the original data for this file\n                    del pixel_data, y\n                    \n                except Exception as e:\n                    print(f\"Skipping {file_name}: {e}\")\n        \n        gc.collect()\n\n    return class_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T04:11:12.862363Z","iopub.status.idle":"2026-01-04T04:11:12.862692Z","shell.execute_reply.started":"2026-01-04T04:11:12.862525Z","shell.execute_reply":"2026-01-04T04:11:12.862544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def augment_audio(y,sr):\n    if np.random.random()>0.5:\n        y=librosa.effects.pitch_shift(y,sr=sr,n_steps=np.random.uniform(-2,2))\n        y = np.asarray(y, dtype=np.float32)\n    noise_amp=0.005*np.random.uniform()*np.amax(y)\n    noise_amp = 0.005 * np.random.uniform() * np.amax(np.abs(y)) if y.size > 0 else 0.0\n    y = y + noise_amp * np.random.normal(size=y.shape).astype(np.float32)\n    return y.astype(np.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def for_single_audio(file):\n    img,y,sr=audio_to_image(file)\n    img_array=np.expand_dims(img_array, axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input=\"/kaggle/input/forest-sound-dataset/forestdataset\"\noutput=\"/kaggle/working/processed\"\nclass_names=convert_dir_to_audio(input,output)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the folder for easy download\nshutil.make_archive(\"/kaggle/working/processed\", 'zip', \"/kaggle/working/forest_processed\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.utils import image_dataset_from_directory, load_img, img_to_array\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:28.280145Z","iopub.execute_input":"2026-01-04T05:06:28.280891Z","iopub.status.idle":"2026-01-04T05:06:28.288160Z","shell.execute_reply.started":"2026-01-04T05:06:28.280864Z","shell.execute_reply":"2026-01-04T05:06:28.287648Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#setting up important var for loading images later on\nBATCH_SIZE=32\nIMG_SIZE=(224,224)\nSEED=42\nEXTRACT_PATH=\"/kaggle/input/forest-sound-spectograph/forest_processed\"\nselected_class=[\"natural sound\",\"unnatural\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:28.427828Z","iopub.execute_input":"2026-01-04T05:06:28.428116Z","iopub.status.idle":"2026-01-04T05:06:28.432045Z","shell.execute_reply.started":"2026-01-04T05:06:28.428091Z","shell.execute_reply":"2026-01-04T05:06:28.431356Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#loading up images and dividing them for the neural net\ntrain_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    image_size=(128, 1000),\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"training\",\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\",\n    color_mode='rgb'\n)\n\n#validation dataset\nvalidation_data=tf.keras.utils.image_dataset_from_directory(\n    EXTRACT_PATH,\n    image_size=(128, 1000),\n    class_names=selected_class,\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=SEED,\n    batch_size=BATCH_SIZE,\n    label_mode=\"binary\",\n    color_mode='rgb'\n)\n#now dividing the validation data set into \ntotal_number_of_batches_in_validation_data=validation_data.cardinality().numpy() #converts total number of batches in the set and converts the tensor into python integer with .numpy function here\nno_of_batches_in_validation_data=total_number_of_batches_in_validation_data//2 #this is a floor division operator\n\n\n#now from the set of batches of the image creating subset into the validation and the test subset\nvalidation=validation_data.take(no_of_batches_in_validation_data)\ntest=validation_data.skip(no_of_batches_in_validation_data)\nprint(f\"Class names: {train_data.class_names}\")\nprint(\"For values:\\n\")\nfor i, class_name in enumerate(train_data.class_names):\n    print(f\"{class_name}:{i}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:30.117000Z","iopub.execute_input":"2026-01-04T05:06:30.117307Z","iopub.status.idle":"2026-01-04T05:06:40.351338Z","shell.execute_reply.started":"2026-01-04T05:06:30.117280Z","shell.execute_reply":"2026-01-04T05:06:40.350783Z"}},"outputs":[{"name":"stdout","text":"Found 8016 files belonging to 2 classes.\nUsing 6413 files for training.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767503196.317802      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1767503196.321715      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Found 8016 files belonging to 2 classes.\nUsing 1603 files for validation.\nClass names: ['natural sound', 'unnatural']\nFor values:\n\nnatural sound:0\n\nunnatural:1\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#now we take pretrained model and train them for our data\n\ndata_augmentation=tf.keras.Sequential([\n     tf.keras.layers.RandomTranslation(\n        height_factor=0.0,  # no vertical shift by default\n        width_factor=0.1,   # shift up to 10% horizontally\n        fill_mode='constant'\n    ),\n\n    # Random vertical shift (frequency axis)\n    tf.keras.layers.RandomTranslation(\n        height_factor=0.1,  # shift up to 10% vertically\n        width_factor=0.0,\n        fill_mode='constant'\n    ),\n\n    # Random brightness adjustment (small)\n    tf.keras.layers.RandomBrightness(factor=0.1),\n\n    # Random contrast adjustment\n    tf.keras.layers.RandomContrast(factor=0.1),\n\n\n\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:40.352457Z","iopub.execute_input":"2026-01-04T05:06:40.352711Z","iopub.status.idle":"2026-01-04T05:06:40.373776Z","shell.execute_reply.started":"2026-01-04T05:06:40.352689Z","shell.execute_reply":"2026-01-04T05:06:40.373329Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def binary_model():\n    # 1. Define Input\n    inputs = tf.keras.Input(shape=(128, 1000, 3))\n    x = data_augmentation(inputs)\n    x = layers.Resizing(224, 224)(x)\n\n\n    base_model = tf.keras.applications.EfficientNetB0(\n        input_shape=(224, 224, 3),\n        include_top=False,\n        weights=\"imagenet\"\n    )\n    base_model.trainable = False\n    \n    x = base_model(x, training=False) # training=False keeps BatchNormalization in inference mode\n\n    # 5. The \"Head\"\n    x = layers.GlobalMaxPooling2D()(x)\n    x = layers.Dense(256, activation=\"relu\")(x)\n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\n    # 6. Create the Model\n    model = models.Model(inputs=inputs, outputs=outputs)\n    \n    return model\nmodel = binary_model()\n\nmodel.compile(optimizer=\"adam\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])\n\nhistory=model.fit(\n    train_data,\n    validation_data=validation,\n    epochs=50,\n    batch_size=32,\n    callbacks=[\n        EarlyStopping(\n            monitor=\"val_accuracy\", #checks for val_accuracy\n            patience=5,#wait tills 5 epochs\n            restore_best_weights=True,#uses best weight\n        ),\n        ModelCheckpoint(\n            \"best_model.keras\",#givesbest model according to val_accuracy\n            monitor=\"val_accuracy\",\n            save_best_only=True,\n            verbose=1 #prints only certain line of epoch for 1 and for 0 is silence and for 2 is every line\n        )\n    ]\n)\ntest_loss,test_acc=model.evaluate(validation)\nprint(f\"Accuracy for validation dataset: {test_acc:4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:06:40.374717Z","iopub.execute_input":"2026-01-04T05:06:40.375142Z","iopub.status.idle":"2026-01-04T05:14:45.183185Z","shell.execute_reply.started":"2026-01-04T05:06:40.375120Z","shell.execute_reply":"2026-01-04T05:14:45.182425Z"}},"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"E0000 00:00:1767503211.847212      55 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1_1/efficientnetb0_1/block2b_drop_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\nI0000 00:00:1767503214.141865     136 cuda_dnn.cc:529] Loaded cuDNN version 91002\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7264 - loss: 0.9143\nEpoch 1: val_accuracy improved from -inf to 0.77125, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 104ms/step - accuracy: 0.7266 - loss: 0.9125 - val_accuracy: 0.7713 - val_loss: 0.5595\nEpoch 2/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7998 - loss: 0.4208\nEpoch 2: val_accuracy improved from 0.77125 to 0.84250, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.7999 - loss: 0.4205 - val_accuracy: 0.8425 - val_loss: 0.3373\nEpoch 3/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8333 - loss: 0.3759\nEpoch 3: val_accuracy improved from 0.84250 to 0.86125, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8334 - loss: 0.3758 - val_accuracy: 0.8612 - val_loss: 0.3071\nEpoch 4/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8350 - loss: 0.3765\nEpoch 4: val_accuracy improved from 0.86125 to 0.86625, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 89ms/step - accuracy: 0.8350 - loss: 0.3763 - val_accuracy: 0.8662 - val_loss: 0.3069\nEpoch 5/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.8346 - loss: 0.3572\nEpoch 5: val_accuracy did not improve from 0.86625\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 87ms/step - accuracy: 0.8348 - loss: 0.3570 - val_accuracy: 0.8238 - val_loss: 0.4123\nEpoch 6/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8205 - loss: 0.4086\nEpoch 6: val_accuracy did not improve from 0.86625\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8206 - loss: 0.4082 - val_accuracy: 0.8625 - val_loss: 0.3216\nEpoch 7/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8414 - loss: 0.3507\nEpoch 7: val_accuracy improved from 0.86625 to 0.88500, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 93ms/step - accuracy: 0.8415 - loss: 0.3506 - val_accuracy: 0.8850 - val_loss: 0.2934\nEpoch 8/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8505 - loss: 0.3424\nEpoch 8: val_accuracy improved from 0.88500 to 0.89000, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8505 - loss: 0.3422 - val_accuracy: 0.8900 - val_loss: 0.2788\nEpoch 9/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8480 - loss: 0.3267\nEpoch 9: val_accuracy did not improve from 0.89000\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - accuracy: 0.8481 - loss: 0.3266 - val_accuracy: 0.8712 - val_loss: 0.2832\nEpoch 10/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8475 - loss: 0.3312\nEpoch 10: val_accuracy did not improve from 0.89000\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8476 - loss: 0.3309 - val_accuracy: 0.8637 - val_loss: 0.3272\nEpoch 11/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8593 - loss: 0.3202\nEpoch 11: val_accuracy did not improve from 0.89000\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8593 - loss: 0.3200 - val_accuracy: 0.8775 - val_loss: 0.2869\nEpoch 12/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8636 - loss: 0.3128\nEpoch 12: val_accuracy improved from 0.89000 to 0.89375, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8636 - loss: 0.3127 - val_accuracy: 0.8938 - val_loss: 0.2503\nEpoch 13/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8569 - loss: 0.3087\nEpoch 13: val_accuracy did not improve from 0.89375\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8569 - loss: 0.3087 - val_accuracy: 0.8600 - val_loss: 0.3091\nEpoch 14/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8707 - loss: 0.2977\nEpoch 14: val_accuracy did not improve from 0.89375\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8707 - loss: 0.2977 - val_accuracy: 0.8850 - val_loss: 0.2717\nEpoch 15/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8712 - loss: 0.2883\nEpoch 15: val_accuracy did not improve from 0.89375\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8712 - loss: 0.2882 - val_accuracy: 0.8725 - val_loss: 0.2810\nEpoch 16/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8706 - loss: 0.2904\nEpoch 16: val_accuracy did not improve from 0.89375\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8706 - loss: 0.2903 - val_accuracy: 0.8875 - val_loss: 0.2784\nEpoch 17/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8608 - loss: 0.3130\nEpoch 17: val_accuracy improved from 0.89375 to 0.90500, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8609 - loss: 0.3128 - val_accuracy: 0.9050 - val_loss: 0.2331\nEpoch 18/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8822 - loss: 0.2692\nEpoch 18: val_accuracy did not improve from 0.90500\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8822 - loss: 0.2692 - val_accuracy: 0.8963 - val_loss: 0.2427\nEpoch 19/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8830 - loss: 0.2760\nEpoch 19: val_accuracy did not improve from 0.90500\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8831 - loss: 0.2759 - val_accuracy: 0.8975 - val_loss: 0.2531\nEpoch 20/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8792 - loss: 0.2885\nEpoch 20: val_accuracy did not improve from 0.90500\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8793 - loss: 0.2884 - val_accuracy: 0.9050 - val_loss: 0.2457\nEpoch 21/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8823 - loss: 0.2659\nEpoch 21: val_accuracy improved from 0.90500 to 0.91125, saving model to best_model.keras\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 91ms/step - accuracy: 0.8823 - loss: 0.2659 - val_accuracy: 0.9112 - val_loss: 0.2220\nEpoch 22/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8724 - loss: 0.2827\nEpoch 22: val_accuracy did not improve from 0.91125\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8725 - loss: 0.2825 - val_accuracy: 0.8938 - val_loss: 0.2481\nEpoch 23/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8848 - loss: 0.2740\nEpoch 23: val_accuracy did not improve from 0.91125\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8847 - loss: 0.2740 - val_accuracy: 0.9062 - val_loss: 0.2458\nEpoch 24/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8822 - loss: 0.2716\nEpoch 24: val_accuracy did not improve from 0.91125\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8822 - loss: 0.2716 - val_accuracy: 0.8975 - val_loss: 0.2374\nEpoch 25/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8768 - loss: 0.2778\nEpoch 25: val_accuracy did not improve from 0.91125\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8769 - loss: 0.2776 - val_accuracy: 0.8863 - val_loss: 0.2587\nEpoch 26/50\n\u001b[1m200/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.8902 - loss: 0.2562\nEpoch 26: val_accuracy did not improve from 0.91125\n\u001b[1m201/201\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 88ms/step - accuracy: 0.8902 - loss: 0.2562 - val_accuracy: 0.9038 - val_loss: 0.2386\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9197 - loss: 0.2120\nAccuracy for validation dataset: 0.916250\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ===== ADD THIS CELL AFTER model.fit() COMPLETES =====\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ” VERIFYING MODEL BEFORE SAVING\")\nprint(\"=\"*80)\n\n# Check the model we just trained\nprint(f\"Model input shape: {model.input_shape}\")\nprint(f\"Expected:          (None, 128, 1000, 3)\")\n\n# Test with a sample batch\nfor images, labels in train_data.take(1):\n    print(f\"\\nSample batch shape: {images.shape}\")\n    print(f\"Sample batch channels: {images.shape[-1]}\")\n    \n    # Try prediction\n    try:\n        pred = model.predict(images[:1])\n        print(f\"âœ… Model can predict on training data\")\n        print(f\"   Prediction shape: {pred.shape}\")\n    except Exception as e:\n        print(f\"âŒ Model CANNOT predict: {e}\")\n        print(\"   TRAINING DATA AND MODEL DON'T MATCH!\")\n\nif model.input_shape == (None, 128, 1000, 3):\n    print(\"\\nâœ… MODEL ARCHITECTURE IS CORRECT\")\nelse:\n    print(f\"\\nâŒ MODEL ARCHITECTURE IS WRONG!\")\n    print(f\"   Model expects: {model.input_shape}\")\n    print(f\"   But should be: (None, 128, 1000, 3)\")\n    raise ValueError(\"Model has wrong input shape! Fix training code!\")\n\nprint(\"=\"*80 + \"\\n\")\n\n# Now save\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_forest_69.keras\")\n\n# Verify the SAVED file\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ” VERIFYING SAVED FILE\")\nprint(\"=\"*80)\n\ntest_model = tf.keras.models.load_model(\n    \"/kaggle/working/models/audio_forest_69.keras\", \n    compile=False\n)\n\nprint(f\"Saved model input shape: {test_model.input_shape}\")\n\nif test_model.input_shape == (None, 128, 1000, 3):\n    print(\"âœ… SAVED MODEL IS CORRECT - Safe to download!\")\nelse:\n    print(\"âŒ SAVED MODEL IS WRONG!\")\n    raise ValueError(\"Something went wrong during save!\")\n    \nprint(\"=\"*80 + \"\\n\")\n\n# Zip it\nimport shutil\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")\nprint(\"âœ… models.zip created - MODEL IS VERIFIED CORRECT!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:28:13.151220Z","iopub.execute_input":"2026-01-04T05:28:13.151563Z","iopub.status.idle":"2026-01-04T05:28:18.213349Z","shell.execute_reply.started":"2026-01-04T05:28:13.151529Z","shell.execute_reply":"2026-01-04T05:28:18.212568Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nğŸ” VERIFYING MODEL BEFORE SAVING\n================================================================================\nModel input shape: (None, 128, 1000, 3)\nExpected:          (None, 128, 1000, 3)\n\nSample batch shape: (32, 128, 1000, 3)\nSample batch channels: 3\n\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\nâœ… Model can predict on training data\n   Prediction shape: (1, 1)\n\nâœ… MODEL ARCHITECTURE IS CORRECT\n================================================================================\n\n\n================================================================================\nğŸ” VERIFYING SAVED FILE\n================================================================================\nSaved model input shape: (None, 128, 1000, 3)\nâœ… SAVED MODEL IS CORRECT - Safe to download!\n================================================================================\n\nâœ… models.zip created - MODEL IS VERIFIED CORRECT!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"result=model.evaluate(test)\nprint(f\"For the unseen test data of the entire training this model has accuracy of {result[1]:.4f}\\\\\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:14:45.185209Z","iopub.execute_input":"2026-01-04T05:14:45.185455Z","iopub.status.idle":"2026-01-04T05:14:48.366357Z","shell.execute_reply.started":"2026-01-04T05:14:45.185433Z","shell.execute_reply":"2026-01-04T05:14:48.365805Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - accuracy: 0.8997 - loss: 0.2479\nFor the unseen test data of the entire training this model has accuracy of 0.9041\\\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\nmodel.save(\"/kaggle/working/models/audio_forest_3.keras\")\n\n# âœ… VERIFY THE SAVED MODEL\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ” VERIFYING SAVED MODEL\")\nprint(\"=\"*60)\n\ntest_model = tf.keras.models.load_model(\n    \"/kaggle/working/models/audio_forest_1.keras\", \n    compile=False\n)\n\nprint(f\"Model input shape: {test_model.input_shape}\")\nprint(f\"Expected:          (None, 128, 1000, 3)\")\n\nif test_model.input_shape == (None, 128, 1000, 3):\n    print(\"âœ… MODEL IS CORRECT - Safe to download!\")\nelse:\n    print(\"âŒ MODEL IS WRONG - DO NOT DOWNLOAD!\")\n    print(\"   Something went wrong during training!\")\n    raise ValueError(\"Model has wrong input shape!\")\n    \nprint(\"=\"*60 + \"\\n\")\n\n# Now zip it\nimport shutil\nshutil.make_archive(\"/kaggle/working/models\", 'zip', \"/kaggle/working/models\")\nprint(\"âœ… models.zip created and verified!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T05:15:45.746143Z","iopub.execute_input":"2026-01-04T05:15:45.746772Z","iopub.status.idle":"2026-01-04T05:15:49.583115Z","shell.execute_reply.started":"2026-01-04T05:15:45.746742Z","shell.execute_reply":"2026-01-04T05:15:49.582407Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nğŸ” VERIFYING SAVED MODEL\n============================================================\nModel input shape: (None, 128, 1000, 3)\nExpected:          (None, 128, 1000, 3)\nâœ… MODEL IS CORRECT - Safe to download!\n============================================================\n\nâœ… models.zip created and verified!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**No of files:**\n<br>\n1) Fire=336<br>\n2) Logging=455<br>\n3) Natural=1063<br>\n4) Poaching=531<br>\n5) Total= 2385<br>\nSo, threshold accuracy=(531/2385)*100%=22.26%","metadata":{}}]}